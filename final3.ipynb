{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94e9e058-3c0a-49af-bc20-ec6a80128307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3425 images belonging to 2 classes.\n",
      "Found 1156 images belonging to 2 classes.\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soumy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m108/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 282ms/step - accuracy: 0.9199 - loss: 1.7830"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soumy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\contextlib.py:155: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 314ms/step - accuracy: 0.9229 - loss: 1.7660 - val_accuracy: 0.7578 - val_loss: 1.7880 - learning_rate: 2.5000e-04\n",
      "Epoch 2/15\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 293ms/step - accuracy: 0.9645 - loss: 1.3388 - val_accuracy: 0.7933 - val_loss: 1.9031 - learning_rate: 2.5000e-04\n",
      "Epoch 3/15\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 312ms/step - accuracy: 0.9704 - loss: 1.1481 - val_accuracy: 0.7958 - val_loss: 2.0872 - learning_rate: 2.5000e-04\n",
      "Epoch 4/15\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 310ms/step - accuracy: 0.9700 - loss: 1.0137 - val_accuracy: 0.7811 - val_loss: 1.8884 - learning_rate: 2.5000e-04\n",
      "Epoch 5/15\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 303ms/step - accuracy: 0.9765 - loss: 0.9010 - val_accuracy: 0.7682 - val_loss: 1.8721 - learning_rate: 1.2500e-04\n",
      "Epoch 6/15\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 304ms/step - accuracy: 0.9724 - loss: 0.8591 - val_accuracy: 0.7682 - val_loss: 1.7460 - learning_rate: 1.2500e-04\n",
      "Epoch 7/15\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 334ms/step - accuracy: 0.9762 - loss: 0.7986 - val_accuracy: 0.7699 - val_loss: 1.9103 - learning_rate: 1.2500e-04\n",
      "Epoch 8/15\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 348ms/step - accuracy: 0.9762 - loss: 0.7373 - val_accuracy: 0.7682 - val_loss: 1.9303 - learning_rate: 1.2500e-04\n",
      "Epoch 9/15\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 402ms/step - accuracy: 0.9824 - loss: 0.6957 - val_accuracy: 0.7690 - val_loss: 1.9623 - learning_rate: 1.2500e-04\n",
      "Epoch 10/15\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 373ms/step - accuracy: 0.9792 - loss: 0.6647 - val_accuracy: 0.7708 - val_loss: 1.7263 - learning_rate: 6.2500e-05\n",
      "Epoch 11/15\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 313ms/step - accuracy: 0.9776 - loss: 0.6413 - val_accuracy: 0.7734 - val_loss: 1.7032 - learning_rate: 6.2500e-05\n",
      "Epoch 12/15\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 302ms/step - accuracy: 0.9820 - loss: 0.6135 - val_accuracy: 0.7673 - val_loss: 2.2252 - learning_rate: 6.2500e-05\n",
      "Epoch 13/15\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 297ms/step - accuracy: 0.9866 - loss: 0.5929 - val_accuracy: 0.7682 - val_loss: 2.3834 - learning_rate: 6.2500e-05\n",
      "Epoch 14/15\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 291ms/step - accuracy: 0.9836 - loss: 0.5803 - val_accuracy: 0.7690 - val_loss: 1.8508 - learning_rate: 6.2500e-05\n",
      "Epoch 15/15\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 323ms/step - accuracy: 0.9817 - loss: 0.5746 - val_accuracy: 0.7682 - val_loss: 2.0230 - learning_rate: 3.1250e-05\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pytesseract\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input, BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "def extract_aadhar_data(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    _, threshed = cv2.threshold(gray, 127, 255, cv2.THRESH_TRUNC)\n",
    "    text1 = pytesseract.image_to_data(threshed, output_type='data.frame', lang=\"hin+eng\")\n",
    "    text2 = pytesseract.image_to_string(threshed, lang=\"hin+eng\")\n",
    "    text = text1[text1.conf != -1]\n",
    "    lines = text.groupby('block_num')['text'].apply(list)\n",
    "    conf = text.groupby(['block_num'])['conf'].mean()\n",
    "    return text\n",
    "\n",
    "image_path = 'C:\\\\Users\\\\soumy\\\\Desktop\\\\cnn\\\\new_generated_aadharcard_images\\\\1front_contrast_adjusted.jpg'\n",
    "aadhar_data = extract_aadhar_data(image_path)\n",
    "\n",
    "def create_improved_cnn_model():\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(64, 64, 3)))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_regularizer=l2(0.005)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.4))\n",
    "    \n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', kernel_regularizer=l2(0.005)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.4))\n",
    "    \n",
    "    # model.add(Conv2D(128, (3, 3), activation='relu', kernel_regularizer=l2(0.005)))\n",
    "    # model.add(BatchNormalization())\n",
    "    # model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    # model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu', kernel_regularizer=l2(0.005)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.00025), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.3,\n",
    "    zoom_range=0.3,\n",
    "    horizontal_flip=True,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.3,\n",
    "    height_shift_range=0.3\n",
    ")\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "    'dataset_1/training_set',\n",
    "    target_size=(64, 64),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "    'dataset_1/test_set',\n",
    "    target_size=(64, 64),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "improved_model = create_improved_cnn_model()\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=0.000005)\n",
    "checkpoint = ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True, mode='min')\n",
    "\n",
    "improved_model.fit(\n",
    "    training_set,\n",
    "    steps_per_epoch=120,\n",
    "    epochs=15,\n",
    "    validation_data=test_set,\n",
    "    validation_steps=40,\n",
    "    callbacks=[early_stopping, reduce_lr, checkpoint]\n",
    ")\n",
    "\n",
    "improved_model.save('improved_aadhar_cnn_model.keras')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2aece9e8-8d60-411b-b5b1-ed0d1636b8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 59ms/step - accuracy: 0.7773 - loss: 1.6794  \n",
      "Test Loss: 1.7031785249710083\n",
      "Test Accuracy: 0.7733563780784607\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step\n",
      "Prediction: Real\n"
     ]
    }
   ],
   "source": [
    "loaded_model = tf.keras.models.load_model('improved_aadhar_cnn_model.keras')\n",
    "\n",
    "evaluation = loaded_model.evaluate(test_set, steps=120)\n",
    "print(f\"Test Loss: {evaluation[0]}\")\n",
    "print(f\"Test Accuracy: {evaluation[1]}\")\n",
    "\n",
    "sample_image_path = 'C:\\\\Users\\\\soumy\\\\Desktop\\\\cnn\\\\new_generated_aadharcard_images\\\\2front_blurred.jpg'\n",
    "sample_image = Image.open(sample_image_path)\n",
    "sample_image = sample_image.resize((64, 64))\n",
    "sample_image = np.array(sample_image) / 255.0\n",
    "sample_image = np.expand_dims(sample_image, axis=0)\n",
    "\n",
    "prediction = loaded_model.predict(sample_image)\n",
    "print(f\"Prediction: {'Real' if prediction[0][0] > 0.5 else 'Fake'}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
